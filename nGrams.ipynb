{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hesham\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import gensim.models.doc2vec\n",
    "import pickle\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.corpora as corpora\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_clean.csv')\n",
    "with open('./docs.pkl', 'rb') as file:\n",
    "    docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [doc.words for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bi_phraser.pkl', 'wb') as file:\n",
    "    pickle.dump(bigram_mod, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./bi_phraser.pkl', 'rb') as file:\n",
    "#     bigram_mod = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_docs = []\n",
    "for doc in docs:\n",
    "    words = bigram_mod[doc.words]\n",
    "    tags = bigram_mod[doc.tags]\n",
    "    bi_docs.append(TaggedDocument(words, tags))\n",
    "word_list = [doc.words for doc in bi_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./docs_bigram.pkl', 'wb') as file:\n",
    "    pickle.dump(bi_docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tri_phraser.pkl', 'wb') as file:\n",
    "    pickle.dump(trigram_mod, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./tri_phraser.pkl', 'rb') as file:\n",
    "#     trigram_mod = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_docs = []\n",
    "for doc in bi_docs:\n",
    "    words = trigram_mod[doc.words]\n",
    "    tags = trigram_mod[doc.tags]\n",
    "    tri_docs.append(TaggedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./docs_trigram.pkl', 'wb') as file:\n",
    "    pickle.dump(tri_docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# nlp.max_length = 1600000\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./docs_bigram_lemmatized.pkl', 'wb') as file:\n",
    "#     pickle.dump(data_lemmatized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag = ''\n",
    "# if tag in doc.tags:\n",
    "#     f = dict()\n",
    "#     for word in doc.words:\n",
    "#         if not word in f.keys():\n",
    "#             f[word] = 1\n",
    "#         else:\n",
    "#             f[word] = f[word]+1\n",
    "#     display(sorted(f.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_docs = []\n",
    "blacklist = ['product', 'home_page', 'shop', 'order', 'contact', 'sale', 'price', 'navig_menu', 'add_cart', 'onlin', 'search',\n",
    "             'buyer', 'seller', 'save', 'account_sign']\n",
    "for doc in tri_docs:\n",
    "    f = dict()\n",
    "    words = [w for w in doc.words if not w in blacklist]\n",
    "    for word in words:\n",
    "        if not word in f.keys():\n",
    "            f[word] = 1\n",
    "        else:\n",
    "            f[word] = f[word]+1\n",
    "    f = sorted(f.items(), key=lambda x:x[1], reverse=True)\n",
    "    tags = [doc.tags[0]]\n",
    "    tags += [f[0][0], f[1][0], f[2][0]]\n",
    "    proc_docs.append(TaggedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./docs_proc.pkl', 'wb') as file:\n",
    "    pickle.dump(proc_docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['retro', 'planet', 'tin_sign', 'retro', 'decor', 'diner_furnitur', 'vintag', 'sign', 'uniqu_gift_idea', 'free_ship_tax', 'free', 'login', 'wish_list', 'view_cart_checkout', 'blog', 'cool', 'retro', 'gift', 'decor', 'item', 'wall_decor', 'metal', 'sign', 'wall_decal', 'floor', 'decal', 'clock_thermomet', 'sign', 'bundl', 'tryptych', 'neon_sign', 'poster_print', 'wood', 'sign', 'canva_wall_art', 'film_cell', 'gold', 'record', 'sign', 'decal', 'set', 'gift', 'sticker', 'mileston_birthday', 'gift_set', 'gift_basket', 'gift_wrap', 'tape', 'greet_card', 'tag', 'joke', 'gag_gift', 'toy', 'hero', 'geek', 'techi', 'pet_pet_lover', 'keychain', 'button_pin', 'kitchen_dine', 'bar', 'coffe_mug', 'tiki_mug', 'deli', 'serv', 'basket', 'kitchen_linen', 'kitchen_canist', 'jar', 'salt_pepper_shaker', 'glasswar', 'dispens', 'restaurantwar', 'tablewar_kitchen', 'drinkwar', 'bar', 'loung', 'parti', 'home_decor', 'funki', 'stuff', 'doormat', 'refriger_magnet', 'woven_throw', 'blanket', 'decor', 'accessori', 'upcycl', 'ikea', 'decal', 'wall', 'desk', 'phone', 'mirror', 'lamp_light', 'furnitur', 'popcorn_machin', 'toy_toy', 'toy_toy', 'classic', 'toy', 'bobblehead', 'tin_toy', 'uniqu', 'coin_bank', 'retro', 'game_puzzl', 'play_card', 'pretend_play', 'toy', 'parti_favor', 'apparel_shirt_sweatshirt', 'work', 'shirt', 'novelti', 'eyewear', 'bag_purs', 'tote', 'tin', 'tote', 'lunchbox', 'new', 'christma', 'vintag', 'origin', 'top_brand', 'coca_cola', 'chevrolet_ford', 'harley_davidson', 'texaco', 'shell', 'ga_oil', 'dean', 'russo', 'comic_marvel_comic', 'mobil', 'rout', 'dodg_chrysler', 'dino', 'sinclair', 'budweis', 'pepsi_cola', 'guin', 'retro', 'planet', 'star_trek_star_war', 'theme', 'happi_birthday', 'robot', 'rock_roll', 'beer', 'cocktail', 'fair', 'midway', 'arcad', 'circu', 'carniv', 'retro', 'cherri', 'garden', 'backyard', 'barber_shop', 'tiki', 'time', 'coffe_lover', 'soda_pop', 'mexican_fiesta', 'surf', 'cabin', 'wood', 'farm', 'live', 'hunt', 'southern', 'style', 'fish', 'cottag', 'sea', 'bath_beauti', 'peopl', 'charact', 'audrey_hepburn', 'betti_boop', 'elvi', 'love_luci', 'marilyn_monro', 'beatl', 'superhero', 'jame_dean', 'john_wayn', 'three_stoog', 'dad_mom', 'hero', 'wizard', 'room_bathroom', 'dorm', 'game', 'rec_room', 'garag', 'home_gym', 'home_theatr', 'kitchen_laundri', 'room', 'kid_room', 'man_cave', 'mom', 'cave', 'offic', 'decad', 'color', 'black_blue_brown', 'grey', 'green_orang_pink_purpl', 'red_white_yellow', 'special', 'new', 'sale_clearanc', 'trend', 'retroplanet', 'retro', 'sign', 'coffe_mug', 'retro', 'clock', 'kitchen_towel', 'coca_cola', 'tiki_mug', 'tool', 'creat_account', 'account_login_order_histori', 'wish_list', 'affili', 'popular', 'custom_support', 'ship_return_faq', 'wall_decal', 'inform', 'site_map', 'retro', 'planet', 'testimoni', 'term_condit_privaci', 'press', 'contest', 'blog', 'retro', 'museum', 'newslett_sign_newslett', 'click', 'address', 'appoint', 'retro', 'planet', 'otterson', 'nashua', 'regist_trademark', 'vintag', 'vend', 'switch_mobil'], tags=['retroplanet.com', 'retro', 'sign', 'planet'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = []\n",
    "urls = data.url.unique()\n",
    "for idx, row in pd.concat(docs).iterrows():\n",
    "    if row.url not in urls:\n",
    "        removed.append(row.url)\n",
    "df = pd.DataFrame(removed)\n",
    "df.to_csv('removed.csv')\n",
    "\n",
    "print(len(removed))\n",
    "with open('removed.json', 'w') as file:\n",
    "    file.write('{\"removed_sites\" : [')\n",
    "    file.write(','.join(['\"'+v.values[0]+'\"' for idx, v in df.iterrows()]))\n",
    "    file.write(']}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
