{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hesham\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os, re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import gensim.models.doc2vec\n",
    "import pickle\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37092"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data_clean.csv')\n",
    "with open('./docs_trigram.pkl', 'rb') as file:\n",
    "    docs = pickle.load(file)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 9 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hesham\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `min_count` (Attribute will be removed in 4.0.0, use self.vocabulary.min_count instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "with open('./d2v_model_trained.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    print(model.vector_size, model.window, model.min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "strictness = 0.7\n",
    "limit = 10\n",
    "minimum = 3\n",
    "max_depth = 3\n",
    "\n",
    "# def get_recommended(url):\n",
    "#     results = dict()\n",
    "#     depth = 0\n",
    "#     ms = model.docvecs.most_similar(url, topn=limit)\n",
    "#     key = [url]\n",
    "\n",
    "#     filtered = [r for r in ms if r[1] > strictness]\n",
    "#     if(len(filtered) < minimum):\n",
    "#         filtered = ms[:minimum]\n",
    "#     return filtered\n",
    "\n",
    "def get_recommended(url):\n",
    "    results = dict()\n",
    "    depth = 0\n",
    "    keys = [url]\n",
    "    n=1\n",
    "    \n",
    "    results = dict()\n",
    "    while True:\n",
    "        m = model.docvecs.most_similar(keys, topn=n)[-1]\n",
    "        if '.' in m[0]:\n",
    "            if not m[0] in results.keys():\n",
    "                results[m[0]] = m[1]\n",
    "            else:\n",
    "                results[m[0]] = 2 / (1/results[m[0]] + 1/m[1])\n",
    "            if len(results.items()) >= limit or (m[1] < strictness and not len(results) < minimum):\n",
    "                break\n",
    "#         elif depth < max_depth and m[1] > strictness:\n",
    "#             keys.append(m[0])\n",
    "#             depth += 1\n",
    "#             n=0\n",
    "        n += 1\n",
    "    \n",
    "    results = sorted(results.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    filtered = [m for m in results if m[1] > strictness and '.' in m[0]]\n",
    "    \n",
    "    if(len(filtered) < minimum):\n",
    "        filtered = results[:minimum]\n",
    "    elif len(filtered) > limit:\n",
    "        filtered = results[:limit]\n",
    "    return filtered\n",
    "\n",
    "def get_text(url):\n",
    "    return '\"'+str(data.text[data.url == url].values[0].encode(\"utf-8\")).replace('\"', '\"\"')+'\"'\n",
    "\n",
    "def get_label(url):\n",
    "     return '\"'+str(data.label[data.url == url].values[0].encode(\"utf-8\")).replace('\"', '\"\"')+'\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.arange(len(docs))\n",
    "np.random.shuffle(rand)\n",
    "rand = rand[:10]\n",
    "\n",
    "with open('./output.csv', 'w') as file:\n",
    "    file.write('match,url,label,text\\n')\n",
    "    for doc in np.array(docs)[rand]:\n",
    "        url = doc[1][0]\n",
    "        label = get_label(url)\n",
    "        ms = get_recommended(url)\n",
    "        \n",
    "        file.write(','+url+','+label+','+get_text(url)+'\\n')\n",
    "#         if kw:\n",
    "#             k = '-'.join(kw)\n",
    "#             file.write(',,'+k+'\\n')\n",
    "        for m in ms:\n",
    "            file.write('%.2f%%,%s,%s,%s\\n'%(m[1]*100,m[0],get_label(m[0]),get_text(m[0])))\n",
    "        file.write(',,\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "with open('./output.json', 'w') as file:\n",
    "    file.write('{\\n')\n",
    "    for doc in np.array(docs):\n",
    "        url = doc[1][0]\n",
    "        ms = get_recommended(url)\n",
    "        file.write('\"'+url+'\":[')\n",
    "        file.write(','.join(['[\"%s\",%.2f]'%(m[0],m[1]) for m in ms]))\n",
    "        file.write('],\\n')\n",
    "        n += 1\n",
    "        if n % 1000 == 0:\n",
    "            print(n)\n",
    "    file.write('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "urls = df[df.match.isnull()].url[~df.url.isnull()].unique()\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haugenhandgunleather.com\n",
      "eaglescreens.com.au\n",
      "millerscheesehouse.com\n",
      "magicmicusa.com\n",
      "malespecies.com\n",
      "donnabsbakery.com\n",
      "ragen-mendenhall.pixels.com\n",
      "cowdogsaddles.com\n",
      "iserv.net/~gtractor/\n",
      "pitbullmotorcyclelifts.com\n",
      "vtkeepsake.com\n",
      "pacificshaving.com\n",
      "eatmyflames.com\n",
      "carolinacookie.com\n",
      "napaonline.com\n",
      "southerntraditionalfoods.com\n",
      "dahliabarn.com\n",
      "grassrootsmeats.com\n",
      "goodsports.com\n",
      "repeaterstore.com\n",
      "merimask.com\n",
      "accessorygeeks.com\n",
      "equalizerhitch.com\n",
      "shirtmagic.com\n",
      "devittliptakdesign.com\n",
      "discounthairsystems.com\n",
      "schopper.ch\n",
      "enchantingbeads.com\n",
      "greekcity.com\n",
      "bobpaulrarecoins.com\n",
      "headwaymusicaudio.com\n",
      "barcelonabathandbody.com\n",
      "audubonart.com\n",
      "atvspecialtiesinc.com\n",
      "pastperfect.com\n",
      "dollhouse-bedding-drapes.com\n",
      "findgiftclubs.com\n",
      "dogchristmasstockings.com\n",
      "colonialwoodworks.net\n",
      "recordsbymail.com\n",
      "tropicalfruitshop.com\n",
      "swisspanoramashop.com\n",
      "seajunk.com\n",
      "rugsdirect.co.uk\n",
      "bmcglobal.com\n",
      "esthersnc.com\n",
      "sallymitchell.com\n",
      "cookiegluck.com\n",
      "ingridandisabel.com\n",
      "lids.com\n",
      "rayersinc.com\n",
      "cbcshop.ca\n",
      "esmodels.co.uk\n",
      "growjourney.com/\n",
      "gerber-tools.com\n",
      "zenwarrior.com\n",
      "hotelchocolat.com\n",
      "inspirals.co.uk\n",
      "customink.com\n",
      "aljancic.com\n",
      "road2racemodels.co.uk\n",
      "shibui.com\n",
      "frugaldougalsgolf.com\n",
      "toolmarts.com\n",
      "gourmetgarlicgardens.com\n",
      "antiquemattress.com\n",
      "wilmingtongrill.com\n",
      "dyersonline.com\n",
      "wildseedfarms.com\n",
      "swiss.chez.com\n",
      "mycoachonline.com/tennis/index.lasso\n",
      "jacktraps.com\n",
      "madeinjerome.com\n",
      "terrylefler.com\n",
      "kegerator.com\n",
      "amishshowroom.com\n",
      "theuncommondog.com\n",
      "millcrestvintage.com\n",
      "barkeepersfriend.com\n",
      "frontgate.com\n",
      "replicolor.com\n",
      "couragecards.org\n",
      "metalgardendesigns.com\n",
      "vintagetransfers.it\n",
      "collectomania.be\n",
      "wharram.com\n",
      "compressionstore.com\n",
      " Knowledgeable Customer Service Availableby Chat\n",
      "binderbooks.com\n",
      "pbcautomotive.com\n",
      "tias.com/stores/bigeds/\n",
      "jack-wolfskin.com\n",
      "zephyrfarm.com.au\n",
      "sustainableseedco.com\n",
      "babyseeshell.com\n",
      "momandpopcorn.com\n",
      "heatingelementcompany.co.uk\n",
      "woofbed.co.uk\n",
      "unique-eggs.com\n",
      "automotivetools.com\n"
     ]
    }
   ],
   "source": [
    "with open('./output.csv', 'w') as file:\n",
    "    file.write('match,url,label,text\\n')\n",
    "    for u in urls:\n",
    "        print(u)\n",
    "        for doc in np.array(docs):\n",
    "            url = doc[1][0]\n",
    "            if url == u:\n",
    "                ms = get_recommended(url)\n",
    "                file.write(','+url+','+str(data[data.url==url].label.values[0].encode(\"utf-8\"))+','+get_text(url)+'\\n')\n",
    "        #         if kw:\n",
    "        #             k = '-'.join(kw)\n",
    "        #             file.write(',,'+k+'\\n')\n",
    "                for m in ms:\n",
    "#                     print(m)\n",
    "                    file.write('%.2f%%,%s,%s,%s\\n'%(m[1]*100,m[0],str(data[data.url==m[0]].label.values[0].encode(\"utf-8\")),get_text(m[0])))\n",
    "                file.write(',,\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
